{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOHQSLRqBMPRj+XayUye5hE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":25,"metadata":{"id":"GYUxwrqiOx1V","executionInfo":{"status":"ok","timestamp":1756852872600,"user_tz":-300,"elapsed":9903,"user":{"displayName":"Sami Chandoor","userId":"03568707039059025292"}}},"outputs":[],"source":["!pip install -q langchain langchain-community faiss-cpu sentence-transformers chromadb openai tiktoken python-dotenv\n","\n","\n","\n","\n"]},{"cell_type":"code","source":["\n","import langchain, langchain_community\n","print(\"LangChain version:\", langchain.__version__)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xb-SRedPPv36","executionInfo":{"status":"ok","timestamp":1756852872619,"user_tz":-300,"elapsed":11,"user":{"displayName":"Sami Chandoor","userId":"03568707039059025292"}},"outputId":"1beedc8e-1508-41fe-d90a-8de68e2bf0bc"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["LangChain version: 0.3.27\n"]}]},{"cell_type":"code","source":["import json\n","from langchain.schema import Document\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","\n","\n","file_path = \"/content/project_1_publications.json\"\n","\n","\n","# Load JSON file directly from directory\n","with open(file_path, \"r\") as f:\n","    data = json.load(f)\n","\n","# Convert into LangChain Documents\n","docs = []\n","for record in data:\n","    text = f\"Title: {record.get('title', '')}\\n\\nContent: {record.get('content', '')}\"\n","    docs.append(Document(page_content=text, metadata={\"id\": record.get(\"id\", None)}))\n","\n","print(\"Loaded:\", len(docs), \"docs\")\n","\n","# Split into chunks for embeddings\n","splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=120)\n","chunks = splitter.split_documents(docs)\n","print(\"Chunks:\", len(chunks))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GyftORM0YK_t","executionInfo":{"status":"ok","timestamp":1756852872662,"user_tz":-300,"elapsed":36,"user":{"displayName":"Sami Chandoor","userId":"03568707039059025292"}},"outputId":"6f9da1ed-44b3-40a8-e4f2-57e794e2af7d"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded: 35 docs\n","Chunks: 35\n"]}]},{"cell_type":"code","source":["from langchain_community.vectorstores import FAISS\n","from langchain_community.embeddings import HuggingFaceEmbeddings\n","\n","embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n","vectorstore = FAISS.from_documents(chunks, embeddings)\n","\n","retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n"],"metadata":{"id":"l9olz03zZEgb","executionInfo":{"status":"ok","timestamp":1756852874364,"user_tz":-300,"elapsed":1698,"user":{"displayName":"Sami Chandoor","userId":"03568707039059025292"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["query = \" How to Add Memory to RAG Applications and AI Agents\"\n","results = retriever.get_relevant_documents(query)\n","\n","for i, r in enumerate(results, 1):\n","    print(f\"\\n--- Result {i} ---\\n\")\n","    print(r.page_content[:400])  # preview\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zghg9QYoZYBf","executionInfo":{"status":"ok","timestamp":1756852874398,"user_tz":-300,"elapsed":31,"user":{"displayName":"Sami Chandoor","userId":"03568707039059025292"}},"outputId":"cb574f4d-3a07-451c-c7a3-3cbb4f171b56"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Result 1 ---\n","\n","Title: How to Add Memory to RAG Applications and AI Agents\n","\n","Content:\n","\n","--- Result 2 ---\n","\n","Title: How to Build RAG Apps with Pinecone, OpenAI, Langchain and Python\n","\n","Content:\n","\n","--- Result 3 ---\n","\n","Title: Core concepts of Agentic AI and AI agents\n","\n","Content:\n"]}]},{"cell_type":"markdown","source":["# New section"],"metadata":{"id":"EwklViiukQ58"}},{"cell_type":"code","source":["\n","!pip install -q transformers sentence-transformers langchain langchain-huggingface\n","\n","# ==== Imports ====\n","from transformers import pipeline\n","from langchain_huggingface import HuggingFacePipeline\n","from langchain_core.prompts import PromptTemplate\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain_core.output_parsers import StrOutputParser\n","\n","\n","\n","# ==== LLM: open-source (no API key needed) ====\n","# You can switch to \"google/flan-t5-large\" if your runtime is strong enough.\n","gen_pipeline = pipeline(\n","    task=\"text2text-generation\",\n","    model=\"google/flan-t5-base\",\n","    # max_new_tokens = how long the answer can be (adjust as needed)\n","    # temperature = 0 for factual/concise answers\n","    max_new_tokens=256,\n","    temperature=0,\n",")\n","\n","llm = HuggingFacePipeline(pipeline=gen_pipeline)\n","\n","# ==== RAG prompt (answers must come from retrieved context) ====\n","prompt = PromptTemplate.from_template(\n","    \"\"\"You are a helpful assistant that answers ONLY using the provided context.\n","If the answer is not in the context, say: \"I don't know based on the provided documents.\"\n","\n","Question:\n","{question}\n","\n","Context:\n","{context}\n","\n","Answer:\"\"\"\n",")\n","\n","def format_docs(docs):\n","    # Join retrieved chunks into a single context string\n","    return \"\\n\\n\".join(d.page_content for d in docs)\n","\n","# ==== Compose the RAG chain ====\n","# 1) Feed the user's question straight through (RunnablePassthrough)\n","# 2) Use retriever to get context, format it\n","# 3) Fill the prompt -> generate with LLM -> parse string\n","rag_chain = (\n","    {\n","        \"question\": RunnablePassthrough(),\n","        \"context\": retriever | format_docs,\n","    }\n","    | prompt\n","    | llm\n","    | StrOutputParser()\n",")\n","\n","# ==== Run a query ====\n","print(rag_chain.invoke(\"whts this about\"))\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8z6pLdvuZmod","executionInfo":{"status":"ok","timestamp":1756852922624,"user_tz":-300,"elapsed":13120,"user":{"displayName":"Sami Chandoor","userId":"03568707039059025292"}},"outputId":"00dea838-757b-458e-df74-d22b970699d0"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cpu\n","The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Science/Tech\n"]}]}]}